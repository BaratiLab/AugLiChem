{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e52af24e",
   "metadata": {},
   "source": [
    "### Set Path (Won't be needed once `setup.py` is finished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "022cd9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(sys.path[0][:-8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdeea0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')                                                                                                                                                       \n",
    "\n",
    "from rdkit.Chem import Draw\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import roc_auc_score as ras\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f064887f",
   "metadata": {},
   "source": [
    "### Auglichem imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6bcf959",
   "metadata": {},
   "outputs": [],
   "source": [
    "from auglichem.molecule import Compose, RandomAtomMask, RandomBondDelete\n",
    "from auglichem.molecule.data import MoleculeDatasetWrapper\n",
    "from auglichem.molecule.models import GCN, AttentiveFP, GINE, DeepGCN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7125df85",
   "metadata": {},
   "source": [
    "### Set up dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "027e63c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class MoleculeDatasetWrapper in module auglichem.molecule.data._molecule_dataset:\n",
      "\n",
      "class MoleculeDatasetWrapper(MoleculeDataset)\n",
      " |  MoleculeDatasetWrapper(*args, **kwds)\n",
      " |  \n",
      " |  Dataset base class for creating graph datasets.\n",
      " |  See `here <https://pytorch-geometric.readthedocs.io/en/latest/notes/\n",
      " |  create_dataset.html>`__ for the accompanying tutorial.\n",
      " |  \n",
      " |  Args:\n",
      " |      root (string, optional): Root directory where the dataset should be\n",
      " |          saved. (optional: :obj:`None`)\n",
      " |      transform (callable, optional): A function/transform that takes in an\n",
      " |          :obj:`torch_geometric.data.Data` object and returns a transformed\n",
      " |          version. The data object will be transformed before every access.\n",
      " |          (default: :obj:`None`)\n",
      " |      pre_transform (callable, optional): A function/transform that takes in\n",
      " |          an :obj:`torch_geometric.data.Data` object and returns a\n",
      " |          transformed version. The data object will be transformed before\n",
      " |          being saved to disk. (default: :obj:`None`)\n",
      " |      pre_filter (callable, optional): A function that takes in an\n",
      " |          :obj:`torch_geometric.data.Data` object and returns a boolean\n",
      " |          value, indicating whether the data object should be included in the\n",
      " |          final dataset. (default: :obj:`None`)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MoleculeDatasetWrapper\n",
      " |      MoleculeDataset\n",
      " |      torch_geometric.data.dataset.Dataset\n",
      " |      torch.utils.data.dataset.Dataset\n",
      " |      typing.Generic\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, dataset, transform=None, split='scaffold', batch_size=64, num_workers=0, valid_size=0.1, test_size=0.1, aug_time=0, data_path=None, target=None, seed=None)\n",
      " |      Input:\n",
      " |      ---\n",
      " |      dataset (str): One of the datasets available from MoleculeNet\n",
      " |                     (http://moleculenet.ai/datasets-1)\n",
      " |      transform (Compose, OneOf, RandomAtomMask, RandomBondDelete object): transormations\n",
      " |                     to apply to the data at call time.\n",
      " |      split (str, optional default=scaffold): random or scaffold. The splitting strategy\n",
      " |                                              used for train/test/validation set creation.\n",
      " |      batch_size (int, optional default=64): Batch size used in training\n",
      " |      num_workers (int, optional default=0): Number of workers used in loading data\n",
      " |      valid_size (float in [0,1], optional default=0.1): \n",
      " |      test_size (float in [0,1],  optional default=0.1): \n",
      " |      aug_time (int, optional default=1):\n",
      " |      data_path (str, optional default=None): specify path to save/lookup data. Default\n",
      " |                  creates `data_download` directory and stores data there\n",
      " |      target (str, optional, default=None): Target variable\n",
      " |      seed (int, optional, default=None): Random seed to use for reproducibility\n",
      " |      \n",
      " |      \n",
      " |      Output:\n",
      " |      ---\n",
      " |      None\n",
      " |  \n",
      " |  get_data_loaders(self, target=None)\n",
      " |      Returns torch_geometric DataLoaders for train/validation/test splits. These can\n",
      " |      be iterated over and are ideal for training and evaluating models.\n",
      " |      \n",
      " |      Inputs:\n",
      " |      -------\n",
      " |      target (str, optional): Target name to get data loaders for. If None, returns the\n",
      " |                            loaders for the first target. If 'all' returns data for\n",
      " |                            all targets at once, ideal for multitarget trainimg.\n",
      " |      \n",
      " |      Outputs:\n",
      " |      --------\n",
      " |      train/valid/test_loader (DataLoader): Data loaders containing the train, validation\n",
      " |                            and test splits of our data.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __parameters__ = ()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from MoleculeDataset:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Selects an element of self.smiles_data according to the index.\n",
      " |      Edge and node masking are done here for each individual molecule\n",
      " |      \n",
      " |      Input:\n",
      " |      -----------------------------------\n",
      " |      index (int): Index of molecule we would like to augment\n",
      " |      \n",
      " |      Output:\n",
      " |      -----------------------------------\n",
      " |      masked_data (Data object): data that has been augmented with node and edge masking\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      The number of examples in the dataset.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch_geometric.data.dataset.Dataset:\n",
      " |  \n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  download(self)\n",
      " |      Downloads the dataset to the :obj:`self.raw_dir` folder.\n",
      " |  \n",
      " |  get(self, idx: int) -> torch_geometric.data.data.Data\n",
      " |      Gets the data object at index :obj:`idx`.\n",
      " |  \n",
      " |  index_select(self, idx: Union[slice, torch.Tensor, numpy.ndarray, collections.abc.Sequence]) -> 'Dataset'\n",
      " |  \n",
      " |  indices(self) -> collections.abc.Sequence\n",
      " |  \n",
      " |  len(self) -> int\n",
      " |  \n",
      " |  process(self)\n",
      " |      Processes the dataset to the :obj:`self.processed_dir` folder.\n",
      " |  \n",
      " |  shuffle(self, return_perm: bool = False) -> Union[ForwardRef('Dataset'), Tuple[ForwardRef('Dataset'), torch.Tensor]]\n",
      " |      Randomly shuffles the examples in the dataset.\n",
      " |      \n",
      " |      Args:\n",
      " |          return_perm (bool, optional): If set to :obj:`True`, will return\n",
      " |              the random permutation used to shuffle the dataset in addition.\n",
      " |              (default: :obj:`False`)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from torch_geometric.data.dataset.Dataset:\n",
      " |  \n",
      " |  num_edge_features\n",
      " |      Returns the number of features per edge in the dataset.\n",
      " |  \n",
      " |  num_features\n",
      " |      Alias for :py:attr:`~num_node_features`.\n",
      " |  \n",
      " |  num_node_features\n",
      " |      Returns the number of features per node in the dataset.\n",
      " |  \n",
      " |  processed_dir\n",
      " |  \n",
      " |  processed_file_names\n",
      " |      The name of the files to find in the :obj:`self.processed_dir`\n",
      " |      folder in order to skip the processing.\n",
      " |  \n",
      " |  processed_paths\n",
      " |      The filepaths to find in the :obj:`self.processed_dir`\n",
      " |      folder in order to skip the processing.\n",
      " |  \n",
      " |  raw_dir\n",
      " |  \n",
      " |  raw_file_names\n",
      " |      The name of the files to find in the :obj:`self.raw_dir` folder in\n",
      " |      order to skip the download.\n",
      " |  \n",
      " |  raw_paths\n",
      " |      The filepaths to find in order to skip the download.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __class_getitem__(params) from builtins.type\n",
      " |  \n",
      " |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __new__(cls, *args, **kwds)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(MoleculeDatasetWrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd8ad662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function __init__ in module auglichem.molecule.data._molecule_dataset:\n",
      "\n",
      "__init__(self, dataset, transform=None, split='scaffold', batch_size=64, num_workers=0, valid_size=0.1, test_size=0.1, aug_time=0, data_path=None, target=None, seed=None)\n",
      "    Input:\n",
      "    ---\n",
      "    dataset (str): One of the datasets available from MoleculeNet\n",
      "                   (http://moleculenet.ai/datasets-1)\n",
      "    transform (Compose, OneOf, RandomAtomMask, RandomBondDelete object): transormations\n",
      "                   to apply to the data at call time.\n",
      "    split (str, optional default=scaffold): random or scaffold. The splitting strategy\n",
      "                                            used for train/test/validation set creation.\n",
      "    batch_size (int, optional default=64): Batch size used in training\n",
      "    num_workers (int, optional default=0): Number of workers used in loading data\n",
      "    valid_size (float in [0,1], optional default=0.1): \n",
      "    test_size (float in [0,1],  optional default=0.1): \n",
      "    aug_time (int, optional default=1):\n",
      "    data_path (str, optional default=None): specify path to save/lookup data. Default\n",
      "                creates `data_download` directory and stores data there\n",
      "    target (str, optional, default=None): Target variable\n",
      "    seed (int, optional, default=None): Random seed to use for reproducibility\n",
      "    \n",
      "    \n",
      "    Output:\n",
      "    ---\n",
      "    None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(MoleculeDatasetWrapper.__init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a17445e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class RandomAtomMask in module auglichem.molecule._transforms:\n",
      "\n",
      "class RandomAtomMask(BaseTransform)\n",
      " |  RandomAtomMask(p: float = 1.0)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      RandomAtomMask\n",
      " |      BaseTransform\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, p: float = 1.0)\n",
      " |      @param p: the probability of the transform being applied; default value is 1.0\n",
      " |  \n",
      " |  apply_transform(self, mol_graph: torch_geometric.data.data.Data, seed: NoneType) -> torch_geometric.data.data.Data\n",
      " |      Transform that randomly mask atoms given a certain ratio\n",
      " |      @param mol_graph: PyG Data to be augmented\n",
      " |      @param seed: \n",
      " |      @returns: Augmented PyG Data\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseTransform:\n",
      " |  \n",
      " |  __call__(self, mol_graph: torch_geometric.data.data.Data, seed=None) -> torch_geometric.data.data.Data\n",
      " |      @param mol_graph: PyG Data to be augmented\n",
      " |      @param metadata: if set to be a list, metadata about the function execution\n",
      " |          including its name, the source & dest width, height, etc. will be appended to\n",
      " |          the inputted list. If set to None, no metadata will be appended or returned\n",
      " |      @returns: Augmented PyG Data\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseTransform:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(RandomAtomMask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2887799f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: ./data_download/gdb8/qm8.csv\n",
      "DATASET: QM8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21783it [00:00, 22844.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating scaffolds...\n",
      "Generating scaffold 0/21782\n",
      "Generating scaffold 1000/21782\n",
      "Generating scaffold 2000/21782\n",
      "Generating scaffold 3000/21782\n",
      "Generating scaffold 4000/21782\n",
      "Generating scaffold 5000/21782\n",
      "Generating scaffold 6000/21782\n",
      "Generating scaffold 7000/21782\n",
      "Generating scaffold 8000/21782\n",
      "Generating scaffold 9000/21782\n",
      "Generating scaffold 10000/21782\n",
      "Generating scaffold 11000/21782\n",
      "Generating scaffold 12000/21782\n",
      "Generating scaffold 13000/21782\n",
      "Generating scaffold 14000/21782\n",
      "Generating scaffold 15000/21782\n",
      "Generating scaffold 16000/21782\n",
      "Generating scaffold 17000/21782\n",
      "Generating scaffold 18000/21782\n",
      "Generating scaffold 19000/21782\n",
      "Generating scaffold 20000/21782\n",
      "Generating scaffold 21000/21782\n",
      "About to sort in scaffold sets\n"
     ]
    }
   ],
   "source": [
    "# Create transformation\n",
    "transform = Compose([\n",
    "    RandomAtomMask([0.1, 0.3]),\n",
    "    RandomBondDelete([0.1, 0.3])\n",
    "])\n",
    "\n",
    "# Initialize dataset object\n",
    "dataset = MoleculeDatasetWrapper(\"QM8\", data_path=\"./data_download\", transform=transform, batch_size=5)\n",
    "\n",
    "# Get train/valid/test splits as loaders\n",
    "train_loader, valid_loader, test_loader = dataset.get_data_loaders(\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072b1301",
   "metadata": {},
   "source": [
    "### Initialize model with task from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f47e690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model\n",
    "num_outputs = len(dataset.labels.keys())\n",
    "model = AttentiveFP(task=dataset.task, output_dim=num_outputs)\n",
    "\n",
    "# Uncomment the following line to use GPU\n",
    "#model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c48a46f",
   "metadata": {},
   "source": [
    "### Initialize traning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4981f4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(dataset.task == 'classification'):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "elif(dataset.task == 'regression'):\n",
    "    criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e818fdf",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2855ac6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3485it [04:20, 13.38it/s]\n",
      "3485it [07:55,  7.33it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    for bn, data in tqdm(enumerate(train_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        loss = 0.\n",
    "        \n",
    "        # Get prediction for all data\n",
    "        _, pred = model(data)\n",
    "        \n",
    "        # To use GPU, data must be cast to cuda\n",
    "        #_, pred = model(data.cuda())\n",
    "\n",
    "        for idx, t in enumerate(train_loader.dataset.target):\n",
    "            # Get indices where target has a value\n",
    "            good_idx = np.where(data.y[:,idx]!=-999999999)\n",
    "            \n",
    "            # When the data is placed on GPU, target must come back to CPU\n",
    "            #good_idx = np.where(data.y.cpu()[:,idx]!=-999999999)\n",
    "\n",
    "            # Prediction is handled differently for classification and regression\n",
    "            if(train_loader.dataset.task == 'classification'):\n",
    "                current_preds = pred[:,2*(idx):2*(idx+1)][good_idx]\n",
    "                current_labels = data.y[:,idx][good_idx]\n",
    "            elif(train_loader.dataset.task == 'regression'):\n",
    "                current_preds = pred[:,idx][good_idx]\n",
    "                current_labels = data.y[:,idx][good_idx]\n",
    "            \n",
    "            loss += criterion(current_preds, current_labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772cfea1",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c81f966f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, validation=False):\n",
    "    set_str = \"VALIDATION\" if validation else \"TEST\"\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # All targets we're evaluating\n",
    "        target_list = test_loader.dataset.target\n",
    "        \n",
    "        # Dictionaries to keep track of predictions and labels for all targets\n",
    "        all_preds = {target: [] for target in target_list}\n",
    "        all_labels = {target: [] for target in target_list}\n",
    "        \n",
    "        model.eval()\n",
    "        for data in test_loader:\n",
    "            # Get prediction for all data\n",
    "            _, pred = model(data)\n",
    "\n",
    "            # To use GPU, data must be cast to cuda\n",
    "            #_, pred = model(data.cuda())\n",
    "            \n",
    "            for idx, target in enumerate(target_list):\n",
    "                # Get indices where target has a value\n",
    "                good_idx = np.where(data.y[:,idx]!=-999999999)\n",
    "                \n",
    "                # When the data is placed on GPU, target must come back to CPU\n",
    "                #good_idx = np.where(data.y.cpu()[:,idx]!=-999999999)\n",
    "                \n",
    "                # Prediction is handled differently for classification and regression\n",
    "                if(train_loader.dataset.task == 'classification'):\n",
    "                    current_preds = pred[:,2*(idx):2*(idx+1)][good_idx][:,1]\n",
    "                    current_labels = data.y[:,idx][good_idx]\n",
    "                elif(train_loader.dataset.task == 'regression'):\n",
    "                    current_preds = pred[:,idx][good_idx]\n",
    "                    current_labels = data.y[:,idx][good_idx]\n",
    "                \n",
    "                # Save predictions and targets\n",
    "                all_preds[target].extend(list(current_preds.detach().cpu().numpy()))\n",
    "                all_labels[target].extend(list(current_labels.detach().cpu().numpy()))\n",
    "            \n",
    "        scores = {target: None for target in target_list}\n",
    "        for target in target_list:\n",
    "            if(test_loader.dataset.task == 'classification'):\n",
    "                scores[target] = ras(all_labels[target], all_preds[target])\n",
    "                print(\"{0} {1} ROC: {2:.5f}\".format(target, set_str, scores[target]))\n",
    "            elif(test_loader.dataset.task == 'regression'):\n",
    "                scores[target] = mean_squared_error(all_labels[target], all_preds[target],\n",
    "                                                    squared=False)\n",
    "                print(\"{0} {1} RMSE: {2:.5f}\".format(target, set_str, scores[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "092df20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E1-CC2 VALIDATION RMSE: 0.02281\n",
      "E2-CC2 VALIDATION RMSE: 0.02368\n",
      "f1-CC2 VALIDATION RMSE: 0.04655\n",
      "f2-CC2 VALIDATION RMSE: 0.07710\n",
      "E1-PBE0 VALIDATION RMSE: 0.02367\n",
      "E2-PBE0 VALIDATION RMSE: 0.02238\n",
      "f1-PBE0 VALIDATION RMSE: 0.03572\n",
      "f2-PBE0 VALIDATION RMSE: 0.05695\n",
      "E1-CAM VALIDATION RMSE: 0.02191\n",
      "E2-CAM VALIDATION RMSE: 0.02015\n",
      "f1-CAM VALIDATION RMSE: 0.05059\n",
      "f2-CAM VALIDATION RMSE: 0.06832\n",
      "E1-CC2 TEST RMSE: 0.02116\n",
      "E2-CC2 TEST RMSE: 0.02255\n",
      "f1-CC2 TEST RMSE: 0.05046\n",
      "f2-CC2 TEST RMSE: 0.06860\n",
      "E1-PBE0 TEST RMSE: 0.02184\n",
      "E2-PBE0 TEST RMSE: 0.02129\n",
      "f1-PBE0 TEST RMSE: 0.04955\n",
      "f2-PBE0 TEST RMSE: 0.05762\n",
      "E1-CAM TEST RMSE: 0.02058\n",
      "E2-CAM TEST RMSE: 0.01961\n",
      "f1-CAM TEST RMSE: 0.05733\n",
      "f2-CAM TEST RMSE: 0.06859\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, valid_loader, validation=True)\n",
    "evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bc37df",
   "metadata": {},
   "source": [
    "### Model saving/loading example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47a4c105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), \"./saved_models/example_gcn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6cafea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E1-CC2 TEST RMSE: 0.25897\n",
      "E2-CC2 TEST RMSE: 0.20809\n",
      "f1-CC2 TEST RMSE: 0.09380\n",
      "f2-CC2 TEST RMSE: 0.06977\n",
      "E1-PBE0 TEST RMSE: 0.28468\n",
      "E2-PBE0 TEST RMSE: 0.26335\n",
      "f1-PBE0 TEST RMSE: 0.09955\n",
      "f2-PBE0 TEST RMSE: 0.06399\n",
      "E1-CAM TEST RMSE: 0.16943\n",
      "E2-CAM TEST RMSE: 0.23965\n",
      "f1-CAM TEST RMSE: 0.07573\n",
      "f2-CAM TEST RMSE: 0.11401\n"
     ]
    }
   ],
   "source": [
    "# Instantiate new model and evaluate\n",
    "model = AttentiveFP(task=dataset.task, output_dim=num_outputs)\n",
    "evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a559e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E1-CC2 TEST RMSE: 0.02116\n",
      "E2-CC2 TEST RMSE: 0.02255\n",
      "f1-CC2 TEST RMSE: 0.05046\n",
      "f2-CC2 TEST RMSE: 0.06860\n",
      "E1-PBE0 TEST RMSE: 0.02184\n",
      "E2-PBE0 TEST RMSE: 0.02129\n",
      "f1-PBE0 TEST RMSE: 0.04955\n",
      "f2-PBE0 TEST RMSE: 0.05762\n",
      "E1-CAM TEST RMSE: 0.02058\n",
      "E2-CAM TEST RMSE: 0.01961\n",
      "f1-CAM TEST RMSE: 0.05733\n",
      "f2-CAM TEST RMSE: 0.06859\n"
     ]
    }
   ],
   "source": [
    "# Load saved model and evaluate\n",
    "model.load_state_dict(torch.load(\"./saved_models/example_gcn\"))\n",
    "evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d193cb74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
